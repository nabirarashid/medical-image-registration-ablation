# -*- coding: utf-8 -*-
"""evaluation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nO4iyAw-5-_k4hr2IFbjiu5gp_f52leR

seg4_paths_copy.txt file

get_data.py file

train_npy_copy file

convert_one_hot.py file

> from only_affine_model
"""

# ====================================================================
# Cell 1: Dependencies and Basic Setup
# ====================================================================
!pip install psutil scikit-learn seaborn nibabel -q

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from scipy.ndimage import map_coordinates
from sklearn.metrics import accuracy_score
import pandas as pd
from tqdm import tqdm
import time
import psutil
import os
from pathlib import Path
from torch.utils.data import DataLoader
import warnings
warnings.filterwarnings('ignore')

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Set plotting style
plt.style.use('seaborn-v0_8')
sns.set_palette("husl")

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Dependencies loaded. Using device: {device}")

# ====================================================================
# Cell 2: Updated Model Architecture Definitions - FIXED
# ====================================================================

import torch
import torch.nn as nn
import torch.nn.functional as F
from pathlib import Path

class BaselineUNet(nn.Module):
    """Baseline 3D U-Net - matches original training exactly"""
    def __init__(self, in_channels=10, out_channels=3, base_channels=32):
        super(BaselineUNet, self).__init__()

        def conv_block(in_channels, out_channels):
            return nn.Sequential(
                nn.Conv3d(in_channels, out_channels, kernel_size=3, padding=1),
                nn.InstanceNorm3d(out_channels),
                nn.ReLU(inplace=True),
                nn.Conv3d(out_channels, out_channels, kernel_size=3, padding=1),
                nn.InstanceNorm3d(out_channels),
                nn.ReLU(inplace=True),
            )

        def upsample_block(in_channels, out_channels):
            return nn.ConvTranspose3d(in_channels, out_channels, kernel_size=2, stride=2)

        # Encoder
        self.enc1 = conv_block(in_channels, 32)
        self.pool1 = nn.MaxPool3d(kernel_size=2, stride=2)
        self.enc2 = conv_block(32, 64)
        self.pool2 = nn.MaxPool3d(kernel_size=2, stride=2)
        self.enc3 = conv_block(64, 128)
        self.pool3 = nn.MaxPool3d(kernel_size=2, stride=2)
        self.enc4 = conv_block(128, 256)
        self.pool4 = nn.MaxPool3d(kernel_size=2, stride=2)

        # Bottleneck
        self.bottleneck = conv_block(256, 512)

        # Decoder
        self.up4 = upsample_block(512, 256)
        self.dec4 = conv_block(512, 256)
        self.up3 = upsample_block(256, 128)
        self.dec3 = conv_block(256, 128)
        self.up2 = upsample_block(128, 64)
        self.dec2 = conv_block(128, 64)
        self.up1 = upsample_block(64, 32)
        self.dec1 = conv_block(64, 32)

        # Output
        self.out_conv = nn.Conv3d(32, 3, kernel_size=1)

    def forward(self, x):
        # Encoder part
        e1 = self.enc1(x)
        p1 = self.pool1(e1)
        e2 = self.enc2(p1)
        p2 = self.pool2(e2)
        e3 = self.enc3(p2)
        p3 = self.pool3(e3)
        e4 = self.enc4(p3)
        p4 = self.pool4(e4)
        b = self.bottleneck(p4)

        # Decoder part
        up4 = self.up4(b)
        d4 = self.dec4(torch.cat((up4, e4), dim=1))
        up3 = self.up3(d4)
        d3 = self.dec3(torch.cat((up3, e3), dim=1))
        up2 = self.up2(d3)
        d2 = self.dec2(torch.cat((up2, e2), dim=1))
        up1 = self.up1(d2)
        d1 = self.dec1(torch.cat((up1, e1), dim=1))

        deformation_field = self.out_conv(d1)
        return deformation_field


class RegLossUNet(nn.Module):
    """U-Net with regularization and tanh scaling"""
    def __init__(self, in_channels=10, out_channels=3, base_channels=32):
        super(RegLossUNet, self).__init__()

        def conv_block(in_channels, out_channels):
            return nn.Sequential(
                nn.Conv3d(in_channels, out_channels, kernel_size=3, padding=1),
                nn.InstanceNorm3d(out_channels),
                nn.ReLU(inplace=True),
                nn.Conv3d(out_channels, out_channels, kernel_size=3, padding=1),
                nn.InstanceNorm3d(out_channels),
                nn.ReLU(inplace=True),
            )

        def upsample_block(in_channels, out_channels):
            return nn.ConvTranspose3d(in_channels, out_channels, kernel_size=2, stride=2)

        # Same architecture as baseline
        self.enc1 = conv_block(in_channels, 32)
        self.pool1 = nn.MaxPool3d(kernel_size=2, stride=2)
        self.enc2 = conv_block(32, 64)
        self.pool2 = nn.MaxPool3d(kernel_size=2, stride=2)
        self.enc3 = conv_block(64, 128)
        self.pool3 = nn.MaxPool3d(kernel_size=2, stride=2)
        self.enc4 = conv_block(128, 256)
        self.pool4 = nn.MaxPool3d(kernel_size=2, stride=2)

        self.bottleneck = conv_block(256, 512)

        self.up4 = upsample_block(512, 256)
        self.dec4 = conv_block(512, 256)
        self.up3 = upsample_block(256, 128)
        self.dec3 = conv_block(256, 128)
        self.up2 = upsample_block(128, 64)
        self.dec2 = conv_block(128, 64)
        self.up1 = upsample_block(64, 32)
        self.dec1 = conv_block(64, 32)

        # Output layer with initialization for small displacements
        self.out_conv = nn.Conv3d(32, 3, kernel_size=1)
        nn.init.normal_(self.out_conv.weight, 0, 0.01)
        nn.init.zeros_(self.out_conv.bias)

    def forward(self, x):
        # Same forward pass as baseline
        e1 = self.enc1(x)
        p1 = self.pool1(e1)
        e2 = self.enc2(p1)
        p2 = self.pool2(e2)
        e3 = self.enc3(p2)
        p3 = self.pool3(e3)
        e4 = self.enc4(p3)
        p4 = self.pool4(e4)
        b = self.bottleneck(p4)

        up4 = self.up4(b)
        d4 = self.dec4(torch.cat((up4, e4), dim=1))
        up3 = self.up3(d4)
        d3 = self.dec3(torch.cat((up3, e3), dim=1))
        up2 = self.up2(d3)
        d2 = self.dec2(torch.cat((up2, e2), dim=1))
        up1 = self.up1(d2)
        d1 = self.dec1(torch.cat((up1, e1), dim=1))

        deformation_field = self.out_conv(d1)
        # Key difference: tanh scaling for controlled deformation
        deformation_field = torch.tanh(deformation_field) * 0.3
        return deformation_field


class AffineParameterPredictor(nn.Module):
    """3D CNN for predicting affine transformation parameters"""
    def __init__(self, in_channels=10):
        super(AffineParameterPredictor, self).__init__()

        # Memory-efficient encoder
        self.encoder = nn.Sequential(
            nn.Conv3d(in_channels, 32, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool3d(2),
            nn.Conv3d(32, 64, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool3d(2),
            nn.AdaptiveAvgPool3d(1)
        )

        # FC layers
        self.fc = nn.Sequential(
            nn.Linear(64, 32),
            nn.ReLU(inplace=True),
            nn.Linear(32, 12)  # 12 affine parameters
        )

        self._init_weights()

    def _init_weights(self):
        # Initialize to output identity transformation
        with torch.no_grad():
            identity = torch.tensor([1,0,0,0, 0,1,0,0, 0,0,1,0], dtype=torch.float32)
            self.fc[-1].weight.data.zero_()
            self.fc[-1].bias.data.copy_(identity)

    def forward(self, x):
        features = self.encoder(x)
        features = features.view(features.size(0), -1)
        affine_params = self.fc(features)
        return affine_params.view(-1, 3, 4)  # Reshape to 3x4 affine matrix


class AffineSpatialTransformer(nn.Module):
    """Applies affine transformation to input tensor"""
    def __init__(self, size):
        super(AffineSpatialTransformer, self).__init__()
        D, H, W = size

        # Create normalized coordinate grid
        z, y, x = torch.meshgrid(
            torch.linspace(-1, 1, D),
            torch.linspace(-1, 1, H),
            torch.linspace(-1, 1, W),
            indexing="ij"
        )
        grid = torch.stack([x, y, z], dim=-1)
        self.register_buffer("grid", grid.view(1, D, H, W, 3))

    def forward(self, x, affine):
        B, C, D, H, W = x.shape

        # Expand grid for batch
        grid = self.grid.expand(B, -1, -1, -1, -1)
        ones = torch.ones(B, D, H, W, 1, device=x.device)
        grid_h = torch.cat([grid, ones], dim=-1)  # Homogeneous coordinates

        # Apply affine transformation
        grid_affine = torch.matmul(grid_h.view(B, -1, 4), affine.transpose(1, 2))
        grid_affine = grid_affine.view(B, D, H, W, 3)

        # Clamp to valid range
        grid_affine = torch.clamp(grid_affine, -1.1, 1.1)

        warped = F.grid_sample(x, grid_affine, mode="bilinear", align_corners=False, padding_mode="border")
        return warped


class AffineRegistrationModel(nn.Module):
    """Complete affine registration model"""
    def __init__(self, in_channels=10):
        super(AffineRegistrationModel, self).__init__()
        self.predictor = AffineParameterPredictor(in_channels)
        self.spatial_transformer = AffineSpatialTransformer((128, 128, 128))

    def forward(self, moving, fixed):
        # Concatenate moving and fixed images
        input_pair = torch.cat([moving, fixed], dim=1)
        # Predict affine parameters
        affine_params = self.predictor(input_pair)
        # Apply transformation to moving image
        registered = self.spatial_transformer(moving, affine_params)
        return registered, affine_params


class SpatialTransformer(nn.Module):
    """Dense STN for 3D one-hot maps with dynamic identity grid"""
    def __init__(self, size=None, device='cpu'):
        super().__init__()
        self.size = size
        if size is not None:
            D, H, W = size
            lin_z = torch.linspace(-1, 1, D, device=device)
            lin_y = torch.linspace(-1, 1, H, device=device)
            lin_x = torch.linspace(-1, 1, W, device=device)
            zz, yy, xx = torch.meshgrid(lin_z, lin_y, lin_x, indexing='ij')
            id_grid = torch.stack((xx, yy, zz), dim=-1)
            self.register_buffer('id_grid', id_grid.unsqueeze(0))
        else:
            self.register_buffer('id_grid', None)

    def forward(self, moving, flow):
        B, C, D, H, W = moving.shape
        flow = flow.permute(0, 2, 3, 4, 1)

        # Create dynamic grid if not pre-computed or size doesn't match
        if self.id_grid is None or self.id_grid.shape[1:4] != (D, H, W):
            lin_z = torch.linspace(-1, 1, D, device=moving.device)
            lin_y = torch.linspace(-1, 1, H, device=moving.device)
            lin_x = torch.linspace(-1, 1, W, device=moving.device)
            zz, yy, xx = torch.meshgrid(lin_z, lin_y, lin_x, indexing='ij')
            id_grid = torch.stack((xx, yy, zz), dim=-1).unsqueeze(0)
        else:
            id_grid = self.id_grid

        grid = id_grid.expand(B, -1, -1, -1, -1)
        warped_grid = grid + flow
        warped_grid = torch.clamp(warped_grid, -1.1, 1.1)

        warped = F.grid_sample(
            moving, warped_grid,
            mode='bilinear',
            padding_mode='border',
            align_corners=False
        )
        return warped


class FullEnhancedModel(nn.Module):
    """Combined affine + non-rigid registration model"""
    def __init__(self, volume_size=(128, 128, 128), num_channels=5):
        super(FullEnhancedModel, self).__init__()
        # Affine predictor takes concatenated input (moving + fixed)
        self.affine_predictor = AffineParameterPredictor(in_channels=num_channels * 2)  # 10 channels
        self.affine_stn = AffineSpatialTransformer(volume_size)

        # UNet also takes concatenated input (affine_warped + fixed)
        self.unet = RegLossUNet(in_channels=num_channels * 2, out_channels=3)  # 10 channels input

        # Spatial transformer for non-rigid deformation
        self.nonrigid_stn = SpatialTransformer(volume_size)

    def forward(self, moving, fixed):
        # First apply affine registration
        x_affine_input = torch.cat([moving, fixed], dim=1)  # 5+5=10 channels
        affine_matrix = self.affine_predictor(x_affine_input)
        source_affine = self.affine_stn(moving, affine_matrix)

        # Then apply non-rigid registration
        x_unet_input = torch.cat([source_affine, fixed], dim=1)  # 5+5=10 channels
        deformation_field = self.unet(x_unet_input)
        warped_source = self.nonrigid_stn(source_affine, deformation_field)

        return warped_source, affine_matrix, deformation_field, source_affine


# ====================================================================
# Baseline with only UNet + STN (matches your baseline training code)
# ====================================================================
class BaselineRegistrationModel(nn.Module):
    """Simple registration model - UNet + STN only (for baseline)"""
    def __init__(self, size=(128, 128, 128), num_channels=5):
        super().__init__()
        self.unet = BaselineUNet(in_channels=num_channels * 2, out_channels=3)
        self.stn = SpatialTransformer(size)

    def forward(self, source, target):
        x_input = torch.cat([source, target], dim=1)
        deformation_field = self.unet(x_input)
        warped_source = self.stn(source, deformation_field)
        return warped_source, deformation_field


# ====================================================================
# RegLoss with UNet + STN (matches your reg loss training code)
# ====================================================================
class RegLossRegistrationModel(nn.Module):
    """Registration model with regularization losses - UNet + STN"""
    def __init__(self, size=(128, 128, 128), num_channels=5):
        super().__init__()
        self.unet = RegLossUNet(in_channels=num_channels * 2, out_channels=3)
        self.stn = SpatialTransformer(size)

    def forward(self, source, target):
        x_input = torch.cat([source, target], dim=1)
        deformation_field = self.unet(x_input)
        warped_source = self.stn(source, deformation_field)
        return warped_source, deformation_field


# ====================================================================
# Combined Model (matches your affine + non-rigid training code)
# ====================================================================
class CombinedRegistrationModel(nn.Module):
    """Combined affine + non-rigid registration model"""
    def __init__(self, size=(128, 128, 128), num_channels=5):
        super().__init__()
        self.affine_predictor = AffineParameterPredictor(in_channels=num_channels * 2)
        self.affine_stn = AffineSpatialTransformer(size)
        self.unet = RegLossUNet(in_channels=num_channels * 2, out_channels=3)
        self.nonrigid_stn = SpatialTransformer(size)

    def forward(self, source, target):
        # First apply affine registration
        x_affine_input = torch.cat([source, target], dim=1)
        affine_matrix = self.affine_predictor(x_affine_input)
        source_affine = self.affine_stn(source, affine_matrix)

        # Then apply non-rigid registration
        x_unet_input = torch.cat([source_affine, target], dim=1)
        deformation_field = self.unet(x_unet_input)
        warped_source = self.nonrigid_stn(source_affine, deformation_field)

        return warped_source, affine_matrix, deformation_field, source_affine


print("Updated model architectures defined successfully!")

# ====================================================================
# Cell 3: Updated Model Loading Functions - FIXED
# ====================================================================

def load_baseline_model(checkpoint_path, device):
    """Load baseline U-Net model from checkpoint"""
    try:
        print(f"Loading Baseline U-Net: {checkpoint_path}")
        checkpoint = torch.load(checkpoint_path, map_location=device)

        # Extract config with proper defaults
        config = checkpoint.get("model_config", {})
        in_channels = config.get("in_channels", 10)
        out_channels = config.get("out_channels", 3)
        size = config.get("size", (128, 128, 128))

        # Create the combined baseline model (UNet + STN)
        model = BaselineRegistrationModel(size=size, num_channels=5)

        # Handle different checkpoint formats
        if "model_state_dict" in checkpoint and "stn_state_dict" in checkpoint:
            # Load UNet and STN separately
            model.unet.load_state_dict(checkpoint["model_state_dict"])
            model.stn.load_state_dict(checkpoint["stn_state_dict"])
        elif "model_state_dict" in checkpoint:
            # Try to load the entire model
            model.load_state_dict(checkpoint["model_state_dict"], strict=False)
        else:
            # Fallback: assume checkpoint is the state dict
            model.load_state_dict(checkpoint, strict=False)

        model.to(device)
        model.eval()
        print(f"   âœ… Baseline loaded (in_channels={in_channels}, out_channels={out_channels})")
        return model

    except Exception as e:
        print(f"   âŒ Failed to load baseline model: {str(e)}")
        return None


def load_regloss_model(checkpoint_path, device):
    """Load regularization loss U-Net model from checkpoint"""
    try:
        print(f"Loading RegLoss U-Net: {checkpoint_path}")
        checkpoint = torch.load(checkpoint_path, map_location=device)

        config = checkpoint.get("model_config", {})
        in_channels = config.get("input_channels", config.get("in_channels", 10))
        out_channels = config.get("output_channels", config.get("out_channels", 3))
        size = config.get("size", (128, 128, 128))

        # Create the combined reg loss model (RegLoss UNet + STN)
        model = RegLossRegistrationModel(size=size, num_channels=5)

        # Handle different checkpoint formats
        if "model_state_dict" in checkpoint and "stn_state_dict" in checkpoint:
            model.unet.load_state_dict(checkpoint["model_state_dict"])
            model.stn.load_state_dict(checkpoint["stn_state_dict"])
        elif "model_state_dict" in checkpoint:
            model.load_state_dict(checkpoint["model_state_dict"], strict=False)
        else:
            model.load_state_dict(checkpoint, strict=False)

        model.to(device)
        model.eval()
        print(f"   âœ… RegLoss loaded (in_channels={in_channels}, out_channels={out_channels})")
        return model

    except Exception as e:
        print(f"   âŒ Failed to load RegLoss model: {str(e)}")
        return None


def load_affine_model(checkpoint_path, device, size=(128,128,128), num_channels=10):
    """Load affine-only registration model from checkpoint"""
    try:
        print(f"Loading Affine Model: {checkpoint_path}")
        checkpoint = torch.load(checkpoint_path, map_location=device)

        config = checkpoint.get('model_config', {})
        in_channels = config.get('in_channels', num_channels)

        model = AffineRegistrationModel(in_channels=in_channels)

        # Flexible loading for different checkpoint formats
        if 'model_state_dict' in checkpoint:
            model.load_state_dict(checkpoint['model_state_dict'], strict=False)
        elif 'predictor_state_dict' in checkpoint and 'spatial_transformer_state_dict' in checkpoint:
            model.predictor.load_state_dict(checkpoint['predictor_state_dict'])
            model.spatial_transformer.load_state_dict(checkpoint['spatial_transformer_state_dict'])
        else:
            model.load_state_dict(checkpoint, strict=False)

        model.to(device)
        model.eval()
        print(f"   âœ… Affine model loaded (in_channels={in_channels})")
        return model

    except Exception as e:
        print(f"   âŒ Failed to load affine model: {str(e)}")
        return None


def load_full_enhanced_model(checkpoint_path, device, size=(128, 128, 128), num_channels=5):
    """Load full enhanced (affine + non-rigid) model from checkpoint"""
    try:
        print(f"Loading Full Enhanced Model: {checkpoint_path}")
        checkpoint = torch.load(checkpoint_path, map_location=device)

        config = checkpoint.get("model_config", {})
        volume_size = config.get("volume_size", size)
        model_num_channels = config.get("num_channels", num_channels)

        # Use the combined model that matches your training architecture
        model = CombinedRegistrationModel(size=volume_size, num_channels=model_num_channels)

        # Load state dict
        if "model_state_dict" in checkpoint:
            model.load_state_dict(checkpoint["model_state_dict"], strict=False)
        else:
            # Try loading individual components if available
            if "affine_predictor_state_dict" in checkpoint:
                model.affine_predictor.load_state_dict(checkpoint["affine_predictor_state_dict"])
            if "affine_stn_state_dict" in checkpoint:
                model.affine_stn.load_state_dict(checkpoint["affine_stn_state_dict"])
            if "unet_state_dict" in checkpoint:
                model.unet.load_state_dict(checkpoint["unet_state_dict"])
            if "nonrigid_stn_state_dict" in checkpoint:
                model.nonrigid_stn.load_state_dict(checkpoint["nonrigid_stn_state_dict"])

        model.to(device)
        model.eval()
        print(f"   âœ… Full enhanced loaded (volume_size={volume_size}, num_channels={model_num_channels})")
        return model

    except Exception as e:
        print(f"   âŒ Failed to load full enhanced model: {str(e)}")
        return None


def get_model_configs():
    """Get updated model configurations"""
    return [
        {
            'name': 'Baseline UNet',
            'description': 'Standard 3D U-Net baseline',
            'path': '/content/drive/MyDrive/segmentation-project/original_unet_model.pth',
            'loader': load_baseline_model,
            'color': '#e74c3c',  # Red
            'type': 'baseline'
        },
        {
            'name': 'RegLoss UNet',
            'description': 'U-Net with regularization losses',
            'path': '/content/drive/MyDrive/segmentation-project/regloss_model_latest.pth',
            'loader': load_regloss_model,
            'color': '#3498db',  # Blue
            'type': 'regloss_only'
        },
        {
            'name': 'Affine Only',
            'description': 'Affine transformation only',
            'path': '/content/drive/MyDrive/segmentation-project/affine_unet_model.pth',
            'loader': load_affine_model,
            'color': '#f39c12',  # Orange
            'type': 'affine_only'
        },
        {
            'name': 'Full Enhanced',
            'description': 'Affine + RegLoss combined',
            'path': '/content/drive/MyDrive/segmentation-project/fully_enhanced_model.pth',
            'loader': load_full_enhanced_model,
            'color': '#27ae60',  # Green
            'type': 'full_enhanced'
        }
    ]

print("Updated model loading functions defined!")

# ====================================================================
# Cell 4: Test Updated Model Loading
# ====================================================================

print("="*80)
print("TESTING UPDATED MODEL LOADING")
print("="*80)

# Set device - force GPU in Colab
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")

if torch.cuda.is_available():
    print(f"GPU: {torch.cuda.get_device_name(0)}")
    print(f"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
    torch.cuda.empty_cache()  # Clear any existing cache
else:
    print("âš ï¸ GPU not available - using CPU (this will be slow!)")

model_configs = get_model_configs()
loaded_models = []

# Test loading each model
for config in model_configs:
    print(f"\n--- Testing {config['name']} ---")
    if Path(config['path']).exists():
        print(f"âœ… File exists: {config['path']}")

        # Try to load the model
        model = config['loader'](config['path'], device)
        if model is not None:
            config['model'] = model
            loaded_models.append(config)

            # Test model with correct training size (128x128x128)
            try:
                with torch.no_grad():
                    if config['type'] in ['baseline', 'regloss_only']:
                        # These models expect moving and fixed separately at training size
                        dummy_moving = torch.randn(1, 5, 128, 128, 128).to(device)
                        dummy_fixed = torch.randn(1, 5, 128, 128, 128).to(device)
                        outputs = model(dummy_moving, dummy_fixed)
                        print(f"   âœ… Forward pass successful: {len(outputs)} outputs returned")
                        for i, out in enumerate(outputs):
                            if torch.is_tensor(out):
                                print(f"      Output {i}: {out.shape}")

                    elif config['type'] == 'affine_only':
                        # Affine model expects moving and fixed separately at training size
                        dummy_moving = torch.randn(1, 5, 128, 128, 128).to(device)
                        dummy_fixed = torch.randn(1, 5, 128, 128, 128).to(device)
                        registered, affine_params = model(dummy_moving, dummy_fixed)
                        print(f"   âœ… Forward pass successful: ({dummy_moving.shape}, {dummy_fixed.shape}) -> {registered.shape}")
                        print(f"   ðŸ“ Affine params shape: {affine_params.shape}")

                    elif config['type'] == 'full_enhanced':
                        # Full enhanced expects moving and fixed separately at training size
                        dummy_moving = torch.randn(1, 5, 128, 128, 128).to(device)
                        dummy_fixed = torch.randn(1, 5, 128, 128, 128).to(device)
                        outputs = model(dummy_moving, dummy_fixed)
                        print(f"   âœ… Forward pass successful: {len(outputs)} outputs returned")
                        for i, out in enumerate(outputs):
                            if torch.is_tensor(out):
                                print(f"      Output {i}: {out.shape}")

            except Exception as forward_error:
                print(f"   âš ï¸  Forward pass failed: {str(forward_error)}")
                print("   (Model loaded but may need architecture adjustments)")

            print(f"   âœ… {config['name']}: Successfully loaded and tested")

        else:
            print(f"   âŒ {config['name']}: Failed to load")
    else:
        print(f"   âš ï¸  File not found: {config['path']}")

print(f"\n" + "="*60)
print(f"UPDATED MODEL LOADING TEST COMPLETE")
print(f"Successfully loaded and tested: {len(loaded_models)}/{len(model_configs)} models")
print("="*60)

if loaded_models:
    print("\nâœ… LOADED MODELS:")
    for config in loaded_models:
        print(f"   - {config['name']} ({config['type']})")
else:
    print("\nâŒ NO MODELS LOADED - Check file paths and architectures")

# ====================================================================
# Cell 5: Registration Metrics Functions
# ====================================================================

def mean_squared_error(pred, target):
    """Calculate Mean Squared Error between predicted and target images"""
    return torch.mean((pred - target) ** 2).item()

def normalized_cross_correlation(pred, target, epsilon=1e-8):
    """Calculate Normalized Cross Correlation with numerical stability"""
    # Flatten the tensors
    pred_flat = pred.view(pred.size(0), -1)
    target_flat = target.view(target.size(0), -1)

    # Center the data
    pred_mean = torch.mean(pred_flat, dim=1, keepdim=True)
    target_mean = torch.mean(target_flat, dim=1, keepdim=True)

    pred_centered = pred_flat - pred_mean
    target_centered = target_flat - target_mean

    # Calculate correlation
    numerator = torch.sum(pred_centered * target_centered, dim=1)
    pred_std = torch.sqrt(torch.sum(pred_centered ** 2, dim=1) + epsilon)
    target_std = torch.sqrt(torch.sum(target_centered ** 2, dim=1) + epsilon)

    denominator = pred_std * target_std + epsilon
    ncc = numerator / denominator

    return torch.mean(ncc).item()

def structural_similarity_index(pred, target, epsilon=1e-8):
    """Calculate simplified SSIM for 3D volumes"""
    C1 = (0.01) ** 2
    C2 = (0.03) ** 2

    mu1 = torch.mean(pred)
    mu2 = torch.mean(target)

    mu1_sq = mu1 ** 2
    mu2_sq = mu2 ** 2
    mu1_mu2 = mu1 * mu2

    sigma1_sq = torch.mean(pred ** 2) - mu1_sq + epsilon
    sigma2_sq = torch.mean(target ** 2) - mu2_sq + epsilon
    sigma12 = torch.mean(pred * target) - mu1_mu2

    numerator = (2 * mu1_mu2 + C1) * (2 * sigma12 + C2)
    denominator = (mu1_sq + mu2_sq + C1) * (sigma1_sq + sigma2_sq + C2) + epsilon

    ssim = numerator / denominator
    return ssim.item()

def dice_coefficient(pred, target, epsilon=1e-8):
    """Calculate Dice coefficient for segmentation masks"""
    if pred.size(1) > 1:
        dice_scores = []
        for c in range(pred.size(1)):
            pred_c = pred[:, c:c+1]
            target_c = target[:, c:c+1]
            intersection = torch.sum(pred_c * target_c)
            union = torch.sum(pred_c) + torch.sum(target_c)
            dice_c = (2.0 * intersection + epsilon) / (union + epsilon)
            dice_scores.append(dice_c.item())
        return np.mean(dice_scores)
    else:
        intersection = torch.sum(pred * target)
        union = torch.sum(pred) + torch.sum(target)
        dice = (2.0 * intersection + epsilon) / (union + epsilon)
        return dice.item()

def calculate_all_metrics(warped_moving, fixed, deformation_field=None, moving_original=None):
    """Calculate all registration metrics"""
    metrics = {}

    # Basic similarity metrics
    metrics['mse'] = mean_squared_error(warped_moving, fixed)
    metrics['ncc'] = normalized_cross_correlation(warped_moving, fixed)
    metrics['ssim'] = structural_similarity_index(warped_moving, fixed)
    metrics['dice'] = dice_coefficient(warped_moving, fixed)

    # If original moving image provided, calculate improvement
    if moving_original is not None:
        metrics['mse_original'] = mean_squared_error(moving_original, fixed)
        metrics['ncc_original'] = normalized_cross_correlation(moving_original, fixed)
        metrics['dice_original'] = dice_coefficient(moving_original, fixed)

        metrics['mse_improvement'] = metrics['mse_original'] - metrics['mse']
        metrics['ncc_improvement'] = metrics['ncc'] - metrics['ncc_original']
        metrics['dice_improvement'] = metrics['dice'] - metrics['dice_original']

    # Deformation field statistics
    if deformation_field is not None:
        def_stats = {
            'mean_magnitude': torch.mean(torch.norm(deformation_field, dim=1)).item(),
            'max_magnitude': torch.max(torch.norm(deformation_field, dim=1)).item(),
            'std_magnitude': torch.std(torch.norm(deformation_field, dim=1)).item()
        }
        metrics['deformation_stats'] = def_stats

    return metrics

def compute_registration_metrics(moving_img, fixed_img, registered_img):
    """Compute registration metrics comparing original and registered images"""
    try:
        device = moving_img.device
        fixed_img = fixed_img.to(device)
        registered_img = registered_img.to(device)

        # Calculate improvements
        mse_original = torch.mean((moving_img - fixed_img) ** 2).item()
        mse_registered = torch.mean((registered_img - fixed_img) ** 2).item()
        mse_improvement = mse_original - mse_registered

        # NCC calculations with epsilon for stability
        def safe_ncc(img1, img2, eps=1e-8):
            img1_flat = img1.view(-1)
            img2_flat = img2.view(-1)
            mean1 = torch.mean(img1_flat)
            mean2 = torch.mean(img2_flat)
            centered1 = img1_flat - mean1
            centered2 = img2_flat - mean2
            numerator = torch.sum(centered1 * centered2)
            denominator = torch.sqrt(torch.sum(centered1**2) * torch.sum(centered2**2)) + eps
            return (numerator / denominator).item()

        ncc_original = safe_ncc(moving_img, fixed_img)
        ncc_registered = safe_ncc(registered_img, fixed_img)
        ncc_improvement = ncc_registered - ncc_original

        # SSIM approximation
        def safe_ssim(img1, img2, eps=1e-8):
            mu1 = torch.mean(img1)
            mu2 = torch.mean(img2)
            sigma1_sq = torch.var(img1) + eps
            sigma2_sq = torch.var(img2) + eps
            sigma12 = torch.mean((img1 - mu1) * (img2 - mu2))

            C1 = 0.01**2
            C2 = 0.03**2
            ssim = ((2*mu1*mu2 + C1) * (2*sigma12 + C2)) / ((mu1**2 + mu2**2 + C1) * (sigma1_sq + sigma2_sq + C2))
            return ssim.item()

        ssim_original = safe_ssim(moving_img, fixed_img)
        ssim_registered = safe_ssim(registered_img, fixed_img)
        ssim_improvement = ssim_registered - ssim_original

        return {
            'mse_original': mse_original,
            'mse_registered': mse_registered,
            'mse_improvement': mse_improvement,
            'ncc_original': ncc_original,
            'ncc_registered': ncc_registered,
            'ncc_improvement': ncc_improvement,
            'ssim_original': ssim_original,
            'ssim_registered': ssim_registered,
            'ssim_improvement': ssim_improvement
        }

    except Exception as e:
        print(f"Warning: Error computing registration metrics: {e}")
        return {
            'mse_original': 0, 'mse_registered': 0, 'mse_improvement': 0,
            'ncc_original': 0, 'ncc_registered': 0, 'ncc_improvement': 0,
            'ssim_original': 0, 'ssim_registered': 0, 'ssim_improvement': 0
        }

def analyze_deformation_field(deformation_field):
    """Analyze deformation field properties"""
    if deformation_field is None:
        return {
            'max_displacement': 0,
            'mean_displacement': 0,
            'std_displacement': 0,
            'has_field': False
        }

    try:
        displacement_magnitude = torch.norm(deformation_field, dim=1)
        return {
            'max_displacement': torch.max(displacement_magnitude).item(),
            'mean_displacement': torch.mean(displacement_magnitude).item(),
            'std_displacement': torch.std(displacement_magnitude).item(),
            'has_field': True
        }
    except Exception as e:
        print(f"Warning: Error analyzing deformation field: {e}")
        return {
            'max_displacement': 0,
            'mean_displacement': 0,
            'std_displacement': 0,
            'has_field': False
        }

print("Registration metrics functions defined!")

# ====================================================================
# Cell 6: Data Loading (Only After Model Testing)
# ====================================================================

# Only proceed with data loading if models loaded successfully
if loaded_models:
    print("\nModels loaded successfully - proceeding with data setup...")

    # Extract tar file and prepare data paths
    tar_path = '/content/drive/MyDrive/segmentation-project/neurite-oasis.v1.0.tar'
    extract_path = '/content/segmentation_data'
    os.makedirs(extract_path, exist_ok=True)

    print("Setting up data extraction...")

    # Extract if needed
    if not os.path.exists(f"{extract_path}/OASIS_OAS1_0001_MR1"):
        print("Extracting data archive...")
        import subprocess
        result = subprocess.run(['tar', '-xf', tar_path, '-C', extract_path],
                              capture_output=True, text=True)
        if result.returncode == 0:
            print("Data extracted successfully")
        else:
            print(f"Extraction failed: {result.stderr}")
    else:
        print("Data already extracted")

    # Update paths in seg4_paths_copy.txt
    print("Updating paths in seg4_paths_copy.txt...")
    try:
        with open("seg4_paths_copy.txt", "r") as f:
            lines = f.readlines()

        with open("seg4_paths_copy.txt", "w") as f:
            for line in lines:
                updated = line.replace("neurite-oasis.v1.0", "segmentation_data")
                f.write(updated)
        print("Paths updated successfully")
    except FileNotFoundError:
        print("seg4_paths_copy.txt not found - you may need to create this file")

    # Convert to one-hot format
    if os.path.exists("convert_one_hot.py"):
        print("Converting to one-hot format...")
        import subprocess
        result = subprocess.run(['python', 'convert_one_hot.py', '--txt_path', 'seg4_paths_copy.txt'],
                              capture_output=True, text=True)
        if result.returncode == 0:
            print("Conversion completed successfully")
        else:
            print(f"Conversion warning/error: {result.stderr}")
    else:
        print("convert_one_hot.py not found - skipping conversion")

    # Update train_npy_copy.txt with correct paths
    print("Updating paths in train_npy_copy.txt...")
    try:
        with open('train_npy_copy.txt', 'r') as f:
            lines = f.readlines()

        new_lines = [line.replace('neurite-oasis.v1.0/', 'segmentation_data/') for line in lines]

        with open('train_npy_copy.txt', 'w') as f:
            f.writelines(new_lines)
        print("Train paths updated successfully")
    except FileNotFoundError:
        print("train_npy_copy.txt not found - creating dummy file")
        with open('train_npy_copy.txt', 'w') as f:
            f.write("dummy_path_1.npy\ndummy_path_2.npy\n")

    print("Data setup complete!")

else:
    print("No models loaded successfully - skipping data loading for now")
    print("Fix model loading issues first, then run data loading separately")

# ====================================================================
# Cell 7: Dataset Creation and Test Split
# ====================================================================

if loaded_models:
    # Import your data loader
    try:
        from get_data import SegDataset
        print("SegDataset imported successfully")
    except ImportError:
        print("Warning: get_data module not found. Creating dummy dataset class.")
        class SegDataset:
            def __init__(self, file_path, template_path):
                try:
                    with open(file_path, 'r') as f:
                        self.files = [line.strip() for line in f.readlines() if line.strip()]
                except FileNotFoundError:
                    print(f"File {file_path} not found, creating dummy dataset")
                    self.files = [f"dummy_file_{i}.npy" for i in range(20)]
                self.template_path = template_path

            def __len__(self):
                return len(self.files)

            def __getitem__(self, idx):
                # Return dummy data for testing - 5 channel volumes
                return torch.randn(5, 64, 64, 64), torch.randn(5, 64, 64, 64)

    # Data paths
    data_file_path = 'train_npy_copy.txt'
    template_path = '/content/segmentation_data/OASIS_OAS1_0016_MR1/seg4_onehot.npy'

    # Check if template exists
    if not os.path.exists(template_path):
        print("Template file not found! Trying to find alternative...")
        import glob
        possible_templates = glob.glob('/content/segmentation_data/*/seg4_onehot.npy')
        if possible_templates:
            template_path = possible_templates[0]
            print(f"Using alternative template: {template_path}")
        else:
            print("Warning: No template file found! Using dummy path.")
            template_path = "dummy_template.npy"

    print(f"Template file: {template_path}")

    def create_test_split(data_file_path, template_path, test_size=0.3, random_state=42):
        """Create test split from dataset"""
        try:
            with open(data_file_path, 'r') as f:
                all_files = [line.strip() for line in f.readlines() if line.strip()]
        except FileNotFoundError:
            print(f"Warning: {data_file_path} not found. Creating dummy file list.")
            all_files = [f"dummy_file_{i}.npy" for i in range(30)]

        np.random.seed(random_state)
        shuffled_files = np.random.permutation(all_files)
        n_total = len(shuffled_files)
        n_test = min(20, max(5, int(n_total * test_size)))
        test_files = shuffled_files[:n_test]

        print(f"Dataset split: {n_total} total, {len(test_files)} test samples")

        with open('test_split.txt', 'w') as f:
            f.write('\n'.join(test_files))

        test_dataset = SegDataset('test_split.txt', template_path)
        return test_dataset

    # Create test dataset
    if not Path('test_split.txt').exists():
        print("Creating test split...")
        test_dataset = create_test_split(data_file_path, template_path)
    else:
        print("Loading existing test split...")
        test_dataset = SegDataset('test_split.txt', template_path)

    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)
    print(f"Test dataset ready: {len(test_dataset)} samples")

else:
    print("Models not loaded - creating dummy dataset for testing")
    class DummyDataset:
        def __len__(self):
            return 10

        def __getitem__(self, idx):
            return torch.randn(5, 64, 64, 64), torch.randn(5, 64, 64, 64)

    test_dataset = DummyDataset()
    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)
    print("Dummy test dataset created for architecture testing")

# ====================================================================
# Cell 8: Ablation Study Evaluator Class
# ====================================================================

class AblationStudyEvaluator:
    """Ablation study evaluator for registration models"""
    def __init__(self, device):
        self.device = device
        self.results = {}

    def evaluate_model(self, model, model_config, test_loader, max_samples=15):
        """Evaluate a single model with proper registration metrics"""
        model_name = model_config['name']
        model_type = model_config['type']
        print(f"\n--- Evaluating {model_name} ---")

        registration_results = []
        deformation_results = []
        inference_times = []

        with torch.no_grad():
            for i, batch in enumerate(tqdm(test_loader, desc=f"Evaluating {model_name}")):
                if i >= max_samples:
                    break

                try:
                    # Extract data with better batch handling
                    if isinstance(batch, (list, tuple)) and len(batch) >= 2:
                        moving_img = batch[0].to(self.device).float()
                        fixed_img = batch[1].to(self.device).float()
                    else:
                        # Single image batch - create synthetic pair
                        img = batch.to(self.device).float() if torch.is_tensor(batch) else batch[0].to(self.device).float()
                        moving_img = img
                        # Create slight variation for fixed image
                        fixed_img = img + torch.randn_like(img) * 0.05

                    # Ensure proper dimensions [B, C, D, H, W]
                    if moving_img.dim() == 4:
                        moving_img = moving_img.unsqueeze(0)
                    if fixed_img.dim() == 4:
                        fixed_img = fixed_img.unsqueeze(0)

                    # Store originals for comparison
                    original_moving = moving_img.clone()
                    original_fixed = fixed_img.clone()

                    start_time = time.time()

                    # Model-specific inference
                    try:
                        if model_type == 'baseline':
                            # Baseline U-Net: concatenate moving and fixed
                            input_pair = torch.cat([original_moving, original_fixed], dim=1)

                            # Check expected input channels
                            expected_channels = model.enc1[0].in_channels
                            if input_pair.size(1) != expected_channels:
                                # Adjust channels to match model expectations
                                if input_pair.size(1) < expected_channels:
                                    padding = torch.zeros(input_pair.size(0),
                                                        expected_channels - input_pair.size(1),
                                                        *input_pair.shape[2:]).to(self.device)
                                    input_pair = torch.cat([input_pair, padding], dim=1)
                                else:
                                    input_pair = input_pair[:, :expected_channels]

                            deformation_field = model(input_pair)
                            registered_img = self.apply_deformation(original_moving, deformation_field)

                        elif model_type == 'regloss_only':
                            # RegLoss U-Net: same as baseline but with constrained output
                            input_pair = torch.cat([original_moving, original_fixed], dim=1)

                            expected_channels = model.enc1[0].in_channels
                            if input_pair.size(1) != expected_channels:
                                if input_pair.size(1) < expected_channels:
                                    padding = torch.zeros(input_pair.size(0),
                                                        expected_channels - input_pair.size(1),
                                                        *input_pair.shape[2:]).to(self.device)
                                    input_pair = torch.cat([input_pair, padding], dim=1)
                                else:
                                    input_pair = input_pair[:, :expected_channels]

                            deformation_field = model(input_pair)
                            registered_img = self.apply_deformation(original_moving, deformation_field)

                        elif model_type == 'affine_only':
                            # Affine only model - expects single channel input
                            moving_single = original_moving[:, :1] if original_moving.size(1) > 1 else original_moving
                            fixed_single = original_fixed[:, :1] if original_fixed.size(1) > 1 else original_fixed

                            registered_img, affine_params = model(moving_single, fixed_single)
                            deformation_field = None  # No deformation field for affine-only

                        elif model_type == 'full_enhanced':
                            # Full enhanced model - expects single channel input
                            moving_single = original_moving[:, :1] if original_moving.size(1) > 1 else original_moving
                            fixed_single = original_fixed[:, :1] if original_fixed.size(1) > 1 else original_fixed

                            # Get all outputs from full enhanced model
                            model_outputs = model(moving_single, fixed_single)
                            if len(model_outputs) == 4:
                                registered_img, affine_matrix, deformation_field, affine_warped = model_outputs
                            else:
                                # Fallback for different output formats
                                registered_img = model_outputs[0]
                                deformation_field = model_outputs[2] if len(model_outputs) > 2 else None

                    except Exception as model_error:
                        print(f"   Model inference error for sample {i}: {str(model_error)}")
                        continue

                    inference_time = time.time() - start_time
                    inference_times.append(inference_time)

                    # Compute registration metrics
                    reg_metrics = compute_registration_metrics(
                        original_moving, original_fixed, registered_img
                    )
                    registration_results.append(reg_metrics)

                    # Analyze deformation field if available
                    deform_metrics = analyze_deformation_field(deformation_field)
                    deformation_results.append(deform_metrics)

                except Exception as e:
                    print(f"   Error processing sample {i}: {str(e)}")
                    continue

        # Aggregate results
        if registration_results:
            def safe_stats(values):
                if not values or all(v is None or np.isnan(v) for v in values):
                    return 0.0, 0.0
                clean_values = [v for v in values if v is not None and not np.isnan(v)]
                if not clean_values:
                    return 0.0, 0.0
                return float(np.mean(clean_values)), float(np.std(clean_values))

            # Registration metrics
            mse_improvements = [r.get('mse_improvement', 0) for r in registration_results]
            ncc_improvements = [r.get('ncc_improvement', 0) for r in registration_results]
            ssim_improvements = [r.get('ssim_improvement', 0) for r in registration_results]

            # Deformation metrics
            max_displacements = [d.get('max_displacement', 0) for d in deformation_results]
            mean_displacements = [d.get('mean_displacement', 0) for d in deformation_results]

            aggregated_results = {
                'model_name': model_name,
                'model_type': model_type,
                'n_samples': len(registration_results),
                'registration_metrics': {
                    'mse_improvement_mean': safe_stats(mse_improvements)[0],
                    'mse_improvement_std': safe_stats(mse_improvements)[1],
                    'ncc_improvement_mean': safe_stats(ncc_improvements)[0],
                    'ncc_improvement_std': safe_stats(ncc_improvements)[1],
                    'ssim_improvement_mean': safe_stats(ssim_improvements)[0],
                    'ssim_improvement_std': safe_stats(ssim_improvements)[1]
                },
                'deformation_metrics': {
                    'max_displacement_mean': safe_stats(max_displacements)[0],
                    'max_displacement_std': safe_stats(max_displacements)[1],
                    'mean_displacement_mean': safe_stats(mean_displacements)[0],
                    'mean_displacement_std': safe_stats(mean_displacements)[1]
                },
                'computational_metrics': {
                    'inference_time_mean': float(np.mean(inference_times)) if inference_times else 0.0,
                    'inference_time_std': float(np.std(inference_times)) if inference_times else 0.0,
                    'inference_time_median': float(np.median(inference_times)) if inference_times else 0.0
                },
                'raw_results': {
                    'registration': registration_results,
                    'deformation': deformation_results,
                    'inference_times': inference_times
                }
            }

            print(f"   Completed: {len(registration_results)} samples processed")
            print(f"   MSE improvement: {aggregated_results['registration_metrics']['mse_improvement_mean']:.3f} Â± {aggregated_results['registration_metrics']['mse_improvement_std']:.3f}")
            print(f"   NCC improvement: {aggregated_results['registration_metrics']['ncc_improvement_mean']:.4f} Â± {aggregated_results['registration_metrics']['ncc_improvement_std']:.4f}")

            return aggregated_results
        else:
            print(f"   No successful evaluations for {model_name}")
            return None

    def apply_deformation(self, moving_img, deformation_field):
        """Apply deformation field to moving image"""
        if deformation_field is None:
            return moving_img

        try:
            B, C, D, H, W = moving_img.shape

            if deformation_field.shape[1] != 3:
                print(f"Warning: Expected 3-channel deformation field, got {deformation_field.shape[1]}")
                return moving_img

            # Create identity grid
            grid_d, grid_h, grid_w = torch.meshgrid(
                torch.linspace(-1, 1, D, device=self.device),
                torch.linspace(-1, 1, H, device=self.device),
                torch.linspace(-1, 1, W, device=self.device),
                indexing='ij'
            )
            grid = torch.stack([grid_w, grid_h, grid_d], dim=0).unsqueeze(0)

            # Scale and add deformation
            deformation_scaled = deformation_field * 0.1  # Conservative scaling
            deformed_grid = grid + deformation_scaled

            # Rearrange for grid_sample: [B, D, H, W, 3]
            deformed_grid = deformed_grid.permute(0, 2, 3, 4, 1)

            # Clamp to prevent sampling issues
            deformed_grid = torch.clamp(deformed_grid, -1.1, 1.1)

            # Apply transformation
            registered = F.grid_sample(
                moving_img,
                deformed_grid,
                mode='bilinear',
                padding_mode='border',
                align_corners=False
            )

            return registered

        except Exception as e:
            print(f"   Warning: Deformation application failed ({str(e)}), returning original image")
            return moving_img

    def run_ablation_study(self, model_configs, test_loader, max_samples=15):
        """Run complete ablation study"""
        print("\n" + "="*80)
        print("ABLATION STUDY: REGISTRATION ENHANCEMENT EVALUATION")
        print("="*80)

        all_results = {}
        successful_evaluations = 0

        for config in model_configs:
            if 'model' in config:
                try:
                    result = self.evaluate_model(config['model'], config, test_loader, max_samples)
                    if result:
                        all_results[config['name']] = result
                        successful_evaluations += 1
                        print(f"âœ… {config['name']}: Evaluation completed successfully")
                    else:
                        print(f"âŒ {config['name']}: Evaluation failed")
                except Exception as e:
                    print(f"âŒ {config['name']}: Exception during evaluation - {str(e)}")
            else:
                print(f"â­ï¸ {config['name']}: Model not loaded, skipping")

        print(f"\nAblation study completed! {successful_evaluations}/{len(model_configs)} models evaluated successfully.")

        self.results = all_results
        return all_results

print("Ablation study evaluator class defined!")

# ====================================================================
# Cell 9: Run Model Evaluation (Only if Models Loaded)
# ====================================================================

if loaded_models:
    print(f"\nStarting ablation study with {len(loaded_models)} loaded models...")
    evaluator = AblationStudyEvaluator(device)
    ablation_results = evaluator.run_ablation_study(loaded_models, test_loader, max_samples=10)

    if ablation_results:
        print(f"\nAblation study completed! {len(ablation_results)} models evaluated.")

        # Quick summary
        print("\nQUICK RESULTS SUMMARY:")
        print("-" * 50)
        for name, results in ablation_results.items():
            mse_imp = results['registration_metrics']['mse_improvement_mean']
            ncc_imp = results['registration_metrics']['ncc_improvement_mean']
            inf_time = results['computational_metrics']['inference_time_mean']
            print(f"{name:15s}: MSEâ†‘{mse_imp:+.3f}, NCCâ†‘{ncc_imp:+.4f}, Time:{inf_time*1000:.1f}ms")
    else:
        print("No models were successfully evaluated.")

else:
    print("No models loaded successfully. Cannot run ablation study.")
    print("Please check model loading issues in the earlier cells.")
    ablation_results = {}

print("\nNotebook restructuring complete!")
print("="*60)
print("RESTRUCTURING SUMMARY:")
print("1. âœ… Dependencies and setup (Cell 1)")
print("2. âœ… Model architectures (Cell 2)")
print("3. âœ… Model loading functions (Cell 3)")
print("4. âœ… Model loading test (Cell 4) - RUNS FIRST!")
print("5. âœ… Registration metrics (Cell 5)")
print("6. âœ… Data loading (Cell 6) - Only if models work")
print("7. âœ… Dataset creation (Cell 7)")
print("8. âœ… Ablation evaluator (Cell 8)")
print("9. âœ… Run evaluation (Cell 9)")
print("="*60)